import pandas as pd
import pickle
import sys
import io
from tensorflow.keras.preprocessing.text import Tokenizer 

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# --- AYARLAR ---
KELIME_SAYISI = 50000  # En sÄ±k kullanÄ±lan 50.000 kelimeyi Ã¶ÄŸrenir

print(f"ğŸš€ HAZIRLIK SÃœRECÄ° BAÅLIYOR...")

try:
    print("â³ 'dengeli_veri.csv' okunuyor...")
    df = pd.read_csv('dengeli_veri.csv')
    
    # Metinleri String'e Ã§evirir sÄ±kÄ±ntÄ± olmamasÄ± iÃ§in
    df['Text'] = df['Text'].astype(str)
    
except FileNotFoundError:
    print("âŒ HATA: 'dengeli_veri.csv' yok!")
    sys.exit()

# Tokenizer EÄŸitir
print(f"ğŸ“š SÃ¶zlÃ¼k oluÅŸturuluyor (Bu iÅŸlem veri boyutuna gÃ¶re 1-2 dk sÃ¼rebilir)...")
tokenizer = Tokenizer(num_words=KELIME_SAYISI, oov_token="<OOV>") 
tokenizer.fit_on_texts(df['Text'].values)

print(f"âœ… SÃ¶zlÃ¼k tamamlandÄ±! Toplam kelime hazinesi: {len(tokenizer.word_index)}")

# Kaydet
print("ğŸ’¾ 'tokenizer.pickle' ve 'hazir_veri.csv' kaydediliyor...")
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

df.to_csv('hazir_veri.csv', index=False, encoding='utf-8')

print("\nğŸ‰ HAZIRLIK BÄ°TTÄ°! Åimdi 02 ve 03 numaralÄ± eÄŸitim kodlarÄ±nÄ± Ã§alÄ±ÅŸtÄ±r.")